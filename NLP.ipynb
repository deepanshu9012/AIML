{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5yGgt59xXg1t"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxMBVdK-ZCFy",
        "outputId": "ed774c62-6907-417e-d6fb-16fca682f0dc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"I Love NLP. It's fascinating! Sentence tokenization is the process of splitting text into sentences.\""
      ],
      "metadata": {
        "id": "lyWHqEhQX6Tc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(text)"
      ],
      "metadata": {
        "id": "2T76DFN8YO7G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDbUCQKRZHWw",
        "outputId": "4389a774-2c7c-46da-a71c-e42edfe01f7e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I Love NLP.', \"It's fascinating!\", 'Sentence tokenization is the process of splitting text into sentences.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "2FKJeAP0aC_L"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Tokenization is the process of breaking down text into tokens.\""
      ],
      "metadata": {
        "id": "FaDgzrBraIMc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(text)"
      ],
      "metadata": {
        "id": "ucdMuf9taM8Q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzwrZ4rSaODe",
        "outputId": "2784ccfd-f364-416f-f2ba-829be189434e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'text', 'into', 'tokens', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"\"\"Swami Vivekananda, born Narendranath Datta in 1863 in Calcutta, India, was a profound spiritual leader, philosopher, and social reformer who played a pivotal role in the revitalization of Hinduism and the spread of Indian spiritual wisdom to the Western world. His life and teachings continue to inspire millions of people worldwide. Vivekananda was deeply influenced by his guru, Sri Ramakrishna Paramahamsa, and their profound spiritual connection shaped his mission in life. He was a proponent of Vedanta, emphasizing the oneness of all religions and the universality of spiritual truths. In 1893, he represented Hinduism at the Parliament of the World's Religions in Chicago, where his historic speech began with the words, \"Sisters and brothers of America,\" earning him worldwide recognition and acclaim. Swami Vivekananda's teachings stressed the importance of self-realization, the divinity of the individual soul, and the need for service to humanity. He believed that education was the key to social reform and worked tirelessly to establish the Ramakrishna Mission and Math, institutions that continue to provide education, healthcare, and humanitarian aid.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Pyj6jDX0aXwM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "gjiqp45vbUhS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFirugNsblWv",
        "outputId": "564dfc83-644d-4673-d705-4be8ba4dbca9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Swami Vivekananda, born Narendranath Datta in 1863 in Calcutta, India, was a profound spiritual leader, philosopher, and social reformer who played a pivotal role in the revitalization of Hinduism and the spread of Indian spiritual wisdom to the Western world.', 'His life and teachings continue to inspire millions of people worldwide.', 'Vivekananda was deeply influenced by his guru, Sri Ramakrishna Paramahamsa, and their profound spiritual connection shaped his mission in life.', 'He was a proponent of Vedanta, emphasizing the oneness of all religions and the universality of spiritual truths.', 'In 1893, he represented Hinduism at the Parliament of the World\\'s Religions in Chicago, where his historic speech began with the words, \"Sisters and brothers of America,\" earning him worldwide recognition and acclaim.', \"Swami Vivekananda's teachings stressed the importance of self-realization, the divinity of the individual soul, and the need for service to humanity.\", 'He believed that education was the key to social reform and worked tirelessly to establish the Ramakrishna Mission and Math, institutions that continue to provide education, healthcare, and humanitarian aid.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word = \"studies\"\n",
        "print(\"Stemmed:\", stemmer.stem(word))\n",
        "print(\"Lemmatized:\", lemmatizer.lemmatize(word, pos=\"v\"))  # 'v' for verb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syCTHAsyc-MW",
        "outputId": "f3ce62a2-a074-49a2-92ac-9771fe484b3d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed: studi\n",
            "Lemmatized: study\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n"
      ],
      "metadata": {
        "id": "J9yQi1AGlJKs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"running\", \"studies\", \"leaves\", \"cars\", \"flies\"]\n"
      ],
      "metadata": {
        "id": "ZhMpF8YRlR-u"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stems = [stemmer.stem(word) for word in words]\n"
      ],
      "metadata": {
        "id": "BLeD25C1lTnU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Words: \", words)\n",
        "print(\"Stemmed Words: \", stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciNIfXNclWEt",
        "outputId": "501f8502-0581-4d8c-e3d7-fb957e3f7132"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words:  ['running', 'studies', 'leaves', 'cars', 'flies']\n",
            "Stemmed Words:  ['run', 'studi', 'leav', 'car', 'fli']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "BgXr6Ustl_rK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "AoPWDcnpmCqS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"running\", \"studies\", \"leaves\", \"cars\", \"flies\"]"
      ],
      "metadata": {
        "id": "6vcNJ-vVmG0O"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in words]"
      ],
      "metadata": {
        "id": "ooBRdjURmI84"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Words: \", words)\n",
        "print(\"Stemmed Words: \", lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UQksD7tmVfC",
        "outputId": "84ab2ec8-63a5-405b-912f-b9d445df7f89"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words:  ['running', 'studies', 'leaves', 'cars', 'flies']\n",
            "Stemmed Words:  ['run', 'study', 'leave', 'cars', 'fly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "aG8qGYcUrg3R"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')      # For tokenizing\n",
        "nltk.download('stopwords')  # For stopword removal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vJYoh2Ered3",
        "outputId": "f31d6ca2-7c4f-469c-ba9d-0effd47a0e98"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"Technology is rapidly transforming the world around us. From smartphones and artificial intelligence to space exploration and medical advancements, innovation is changing the way we live and interact. However, with great progress comes great responsibility. It is essential to use technology ethically, ensuring it benefits all of humanity. As we move into the future, embracing innovation while staying grounded in values will shape a better tomorrow.\"\"\"\n",
        "  # Full text goes here\n",
        "\n",
        "# 1. Tokenize into sentences (optional)\n",
        "sentences = sent_tokenize(paragraph)\n",
        "\n",
        "# 2. Initialize stemmer and stopwords\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# 3. Tokenize, remove stopwords, and stem each word\n",
        "processed = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    filtered = [stemmer.stem(word) for word in words if word.lower() not in stop_words and word.isalnum()]\n",
        "    processed.append(\" \".join(filtered))\n",
        "\n",
        "# 4. Print result\n",
        "for i, sent in enumerate(processed, 1):\n",
        "    print(f\"Sentence {i}: {sent}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEYCPLywq0cw",
        "outputId": "b3ee19bf-364c-47ad-d763-997b688844b1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: technolog rapidli transform world around us\n",
            "Sentence 2: smartphon artifici intellig space explor medic advanc innov chang way live interact\n",
            "Sentence 3: howev great progress come great respons\n",
            "Sentence 4: essenti use technolog ethic ensur benefit human\n",
            "Sentence 5: move futur embrac innov stay ground valu shape better tomorrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download resources (only once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4UajnMHu_-D",
        "outputId": "47ace62d-bb76-4db3-b3e6-4e731b96c6c1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"Education is the most powerful weapon which you can use to change the world.\n",
        "It empowers individuals, promotes equality, and drives progress.\n",
        "In the modern era, learning is no longer confined to classrooms.\n",
        "Technology and the internet have made education more accessible than ever before.\"\"\"\n"
      ],
      "metadata": {
        "id": "w9OGZwQNvCil"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = sent_tokenize(paragraph)\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Loop over each sentence\n",
        "for i in range(len(sentences)):\n",
        "    # Word tokenization\n",
        "    words = word_tokenize(sentences[i])\n",
        "\n",
        "    # Lemmatize after removing stopwords\n",
        "    words = [lemmatizer.lemmatize(word) for word in words\n",
        "             if word.lower() not in stopwords.words('english') and word.isalnum()]\n",
        "\n",
        "    # Reconstruct sentence\n",
        "    sentences[i] = ' '.join(words)\n",
        "\n",
        "# Print the final cleaned sentences\n",
        "for i, s in enumerate(sentences, 1):\n",
        "    print(f\"Sentence {i}: {s}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcE0lu7vvFhD",
        "outputId": "ccd151c8-cd8e-4f68-aeeb-4808001dad39"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: Education powerful weapon use change world\n",
            "Sentence 2: empowers individual promotes equality drive progress\n",
            "Sentence 3: modern era learning longer confined classroom\n",
            "Sentence 4: Technology internet made education accessible ever\n"
          ]
        }
      ]
    }
  ]
}